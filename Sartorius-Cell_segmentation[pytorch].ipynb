{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd54709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import cv2\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "import albumentations.pytorch\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.ndimage as ndi\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import PIL\n",
    "\n",
    "import GPUtil\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from skimage.filters import threshold_otsu\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from jupyterthemes import jtplot\n",
    "import warnings\n",
    "\n",
    "#import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edbee5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "jtplot.style(theme='monokai', context='notebook', ticks=True, grid=False)\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#torch.backends.cudnn.benchmark = True\n",
    "#torch.backends.cudnn.enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "718eb63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random(n):\n",
    "    random.seed(n)\n",
    "    np.random.seed(n)\n",
    "    torch.manual_seed(n)\n",
    "    torch.cuda.manual_seed(n)\n",
    "    torch.cuda.manual_seed_all(n)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    os.environ['PYTHONHASHSEED'] = str(n)\n",
    "    \n",
    "set_random(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e54497",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "646192db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './sartorius-cell-instance-segmentation'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 5\n",
    "TRAIN_CSV = f\"{data_dir}/train.csv\"\n",
    "TRAIN_PATH = f\"{data_dir}/train\"\n",
    "TEST_PATH = f\"{data_dir}/test\"\n",
    "WIDTH = 704\n",
    "HEIGHT = 520"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3127c503",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48a9ad18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pictures in train dir: 606 pcs\n",
      "\n",
      "Number of pictures in test dir: 3 pcs\n"
     ]
    }
   ],
   "source": [
    "train_files = sorted(list(Path(TRAIN_PATH).rglob('*png')))\n",
    "test_files = sorted(list(Path(TEST_PATH).rglob('*.png')))\n",
    "print(f'Number of pictures in train dir: {len(train_files)} pcs')\n",
    "print()\n",
    "print(f'Number of pictures in test dir: {len(test_files)} pcs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19108ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 73585 entries, 0 to 73584\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   id                 73585 non-null  object\n",
      " 1   annotation         73585 non-null  object\n",
      " 2   width              73585 non-null  int64 \n",
      " 3   height             73585 non-null  int64 \n",
      " 4   cell_type          73585 non-null  object\n",
      " 5   plate_time         73585 non-null  object\n",
      " 6   sample_date        73585 non-null  object\n",
      " 7   sample_id          73585 non-null  object\n",
      " 8   elapsed_timedelta  73585 non-null  object\n",
      "dtypes: int64(2), object(7)\n",
      "memory usage: 5.1+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>annotation</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>plate_time</th>\n",
       "      <th>sample_date</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>elapsed_timedelta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65518</th>\n",
       "      <td>de504eaa5349</td>\n",
       "      <td>252178 6 252881 8 253585 9 254288 10 254992 11...</td>\n",
       "      <td>704</td>\n",
       "      <td>520</td>\n",
       "      <td>shsy5y</td>\n",
       "      <td>11h30m00s</td>\n",
       "      <td>2019-06-15</td>\n",
       "      <td>shsy5y[diff]_D12-1_Vessel-714_Ph_2</td>\n",
       "      <td>0 days 11:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48139</th>\n",
       "      <td>a76fe4d00355</td>\n",
       "      <td>112202 6 112905 10 113608 11 114314 9 115018 9...</td>\n",
       "      <td>704</td>\n",
       "      <td>520</td>\n",
       "      <td>shsy5y</td>\n",
       "      <td>11h30m00s</td>\n",
       "      <td>2019-06-14</td>\n",
       "      <td>shsy5y[diff]_E10-2_Vessel-714_Ph_3</td>\n",
       "      <td>0 days 11:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37042</th>\n",
       "      <td>836e5872684a</td>\n",
       "      <td>268741 3 269440 10 270138 18 270839 23 271544 ...</td>\n",
       "      <td>704</td>\n",
       "      <td>520</td>\n",
       "      <td>shsy5y</td>\n",
       "      <td>11h30m00s</td>\n",
       "      <td>2019-06-14</td>\n",
       "      <td>shsy5y[diff]_D12-4_Vessel-714_Ph_4</td>\n",
       "      <td>0 days 11:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60537</th>\n",
       "      <td>d09291445961</td>\n",
       "      <td>125348 8 126050 11 126753 13 127457 15 128162 ...</td>\n",
       "      <td>704</td>\n",
       "      <td>520</td>\n",
       "      <td>astro</td>\n",
       "      <td>09h00m00s</td>\n",
       "      <td>2020-09-14</td>\n",
       "      <td>astros[cereb]_F8-2_Vessel-361_Ph_4</td>\n",
       "      <td>0 days 09:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                         annotation  width  \\\n",
       "65518  de504eaa5349  252178 6 252881 8 253585 9 254288 10 254992 11...    704   \n",
       "48139  a76fe4d00355  112202 6 112905 10 113608 11 114314 9 115018 9...    704   \n",
       "37042  836e5872684a  268741 3 269440 10 270138 18 270839 23 271544 ...    704   \n",
       "60537  d09291445961  125348 8 126050 11 126753 13 127457 15 128162 ...    704   \n",
       "\n",
       "       height cell_type plate_time sample_date  \\\n",
       "65518     520    shsy5y  11h30m00s  2019-06-15   \n",
       "48139     520    shsy5y  11h30m00s  2019-06-14   \n",
       "37042     520    shsy5y  11h30m00s  2019-06-14   \n",
       "60537     520     astro  09h00m00s  2020-09-14   \n",
       "\n",
       "                                sample_id elapsed_timedelta  \n",
       "65518  shsy5y[diff]_D12-1_Vessel-714_Ph_2   0 days 11:30:00  \n",
       "48139  shsy5y[diff]_E10-2_Vessel-714_Ph_3   0 days 11:30:00  \n",
       "37042  shsy5y[diff]_D12-4_Vessel-714_Ph_4   0 days 11:30:00  \n",
       "60537  astros[cereb]_F8-2_Vessel-361_Ph_4   0 days 09:00:00  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_df = pd.read_csv(TRAIN_CSV)\n",
    "cell_df.info()\n",
    "cell_df.sample(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c38297",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcb7b591",
   "metadata": {},
   "source": [
    "# Visualize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7907a34a",
   "metadata": {},
   "source": [
    "Create functions to make a mask based on annotations given in train.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0eb168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_decode(mask_rle,shape,color=1):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return \n",
    "    if colors = 1, then:\n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    \n",
    "    s = mask_rle.split()\n",
    "    # get starting pixel and cells's length\n",
    "    start, length = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    start -=1\n",
    "    # Ending pixels\n",
    "    ends = start + length\n",
    "    img = np.zeros((shape[0] * shape[1], shape [2]), dtype=np.float32)\n",
    "    for lo, hi in zip (start, ends):\n",
    "        img[lo : hi] = color\n",
    "    return img.reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602f1c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mask(labels, input_shape, colors=True):\n",
    "    height, width = input_shape\n",
    "    if colors:\n",
    "        mask = np.zeros((height, width, 3))\n",
    "        for label in labels:\n",
    "            mask += rle_decode(label, shape=(height,width,3), color=np.random.rand(3))\n",
    "    else:\n",
    "        mask = np.zeros((height, width, 1))\n",
    "        for label in labels:\n",
    "            mask += rle_decode(label, shape=(height,width, 1))\n",
    "    mask = mask.clip(0,1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cade67f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    #rand_idx = int(np.random.uniform(0, 607))\n",
    "    #im = PIL.Image.open(train_files[rand_idx])\n",
    "    im = PIL.Image.open(train_files[i])\n",
    "    #label = (train_files[rand_idx]).name[:-4]\n",
    "    label = (train_files[i]).stem#name[:-4]\n",
    "    sample_im_df = cell_df[cell_df['id'] == label]\n",
    "    sample_rles = sample_im_df['annotation'].values\n",
    "    sample_masks1 = build_mask(sample_rles,input_shape=(520,704), colors=False)\n",
    "    sample_masks2 = build_mask(sample_rles,input_shape=(520,704), colors=True)\n",
    "    \n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30, 20),)\n",
    "    ax1.imshow(im,cmap='gray')\n",
    "    ax2.imshow(sample_masks1, cmap='gray')\n",
    "    ax3.imshow(im,cmap='gray')\n",
    "    ax3.imshow(sample_masks2,alpha = .2)\n",
    "    \n",
    "    ax1.set_title('Cell id: ' + str(label))\n",
    "    ax2.set_title('Mask for id: ' + str(label))\n",
    "    ax3.set_title('Mask & pic: ' + str(label))\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993f7b6b",
   "metadata": {},
   "source": [
    "Let's take a closer look to the second image. Seems something wrong with a mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78176aa5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 12))\n",
    "im = PIL.Image.open(train_files[1])\n",
    "label = (train_files[1]).stem#name[:-4]\n",
    "sample_im_df = cell_df[cell_df['id'] == label]\n",
    "sample_rles = sample_im_df['annotation'].values\n",
    "mask = build_mask(sample_rles,input_shape=(520,704), colors=False)\n",
    "\n",
    "\n",
    "plt.imshow(im,cmap='gray')\n",
    "plt.imshow(mask,alpha = .4,);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac49645e",
   "metadata": {},
   "source": [
    "Seems here we have an abnormally long continuous lines.\n",
    "\n",
    "The mask is not representative enough.\n",
    "\n",
    "We can take only correct masks to train our model.\n",
    "\n",
    "**Thanks** [Slavko Prytula](https://www.kaggle.com/slavkoprytula/mask-correction-mask-filtering-updated#Utils)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee78156",
   "metadata": {},
   "outputs": [],
   "source": [
    "TH = 40\n",
    "\n",
    "def clean_mask(mask):\n",
    "    '''\n",
    "    Function is called to identify whether the mask is broken\n",
    "    returns True or False state and also returns a mask\n",
    "    '''\n",
    "    mask = mask > threshold_otsu(np.array(mask).astype(np.uint8))\n",
    "    mask = ndi.binary_fill_holes(mask).astype(np.uint8)\n",
    "    \n",
    "    # New code for mask acceptance\n",
    "    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    c = contours[0][:, 0]\n",
    "    diff = c - np.roll(c, 1, 0)\n",
    "    # find horizontal lines longer than threshold\n",
    "    targets = (diff[:, 1] == 0) & (np.abs(diff[:, 0]) >= TH)  \n",
    "    \n",
    "    return mask, (True in targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257d3bba",
   "metadata": {},
   "source": [
    "Re-write a function for building a masks, taking only not broken masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fbda5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mask(labels, input_shape, colors=True):\n",
    "    height, width = input_shape\n",
    "    masks = np.zeros((height, width, 1))\n",
    "    #masks = np.zeros((height,width), dtype=np.uint8)\n",
    "    for i, label in enumerate(labels):\n",
    "        a_mask = rle_decode(label, shape=(height,width, 1))\n",
    "        a_mask = np.array(a_mask) > 0\n",
    "        a_mask, broken_mask = clean_mask(a_mask)\n",
    "        if broken_mask:\n",
    "            continue\n",
    "        masks += a_mask\n",
    "    masks = masks.clip(0,1)\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2718b450",
   "metadata": {},
   "source": [
    "Check the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ad5368",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 12))\n",
    "mask = build_mask(sample_rles,input_shape=(520,704))\n",
    "\n",
    "plt.imshow(im, cmap='gray')\n",
    "plt.imshow(mask,alpha = .4);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef34b90a",
   "metadata": {},
   "source": [
    "# DataSets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b779db80",
   "metadata": {},
   "source": [
    "## Define Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2908e732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NeuroDataSet(Dataset):\n",
    "#     def __init__(self, files, df: pd.core.frame.DataFrame, train:bool, transform=None, use_cache=False):\n",
    "#         super().__init__()\n",
    "#         self.files = sorted(files)\n",
    "#         self.df = df\n",
    "#         self.gb = self.df.groupby('id')\n",
    "#         self.transform = transform\n",
    "#         self.cached_img = []\n",
    "#         self.cached_mask = []\n",
    "#         self.use_cache = use_cache\n",
    "        \n",
    "        \n",
    "#         self.len_ = len(self.files)\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return self.len_\n",
    "    \n",
    "#     def load_sample(self, file):\n",
    "#         img = cv2.imread(str(file))\n",
    "#         img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#         img_id = file.stem\n",
    "#         return img, img_id\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "        \n",
    "#         if not self.use_cache:\n",
    "#             # load image\n",
    "#             image, image_id = self.load_sample(self.files[index])\n",
    "#             annotation_df = self.gb.get_group(image_id)\n",
    "#             #annotation = self.gb.get_group(self.files[index].stem)\n",
    "#             #Create a mask\n",
    "#             annotations = annotation_df['annotation'].values\n",
    "#             mask = build_mask(annotations,input_shape=(520,704), colors=False)\n",
    "            \n",
    "#             if self.transform is not None and self.train:\n",
    "#                 image = self.transform(image = image)[\"image\"]\n",
    "#                 mask = self.transform(image = mask)[\"image\"]\n",
    "\n",
    "#             self.cached_img.append(image)\n",
    "#             self.cached_mask.append(mask)\n",
    "#         else:\n",
    "#             image = self.cached_img[index]\n",
    "#             mask = self.cached_mask[index]\n",
    "                                    \n",
    "#         return image, mask\n",
    "    \n",
    "#     def set_use_cahe(self, use_cache):\n",
    "#         if use_cache:\n",
    "#             self.cached_img = torch.stack(self.cached_img)\n",
    "#             self.cached_mask = torch.stack(self.cached_mask)\n",
    "#         else:\n",
    "#             self.cached_img = []\n",
    "#             self.cached_mask = []\n",
    "#         self.use_cache = use_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651d8564",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuroDataSet(Dataset):\n",
    "    def __init__(self, files, df: pd.core.frame.DataFrame, train:bool, transform=None):\n",
    "        super().__init__()\n",
    "        self.files = sorted(files)\n",
    "        self.df = df\n",
    "        self.gb = self.df.groupby('id')\n",
    "        self.transform = transform      \n",
    "        \n",
    "        self.len_ = len(self.files)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len_\n",
    "    \n",
    "    #@functools.lru_cache(12)\n",
    "    def load_sample(self, file):\n",
    "        img = cv2.imread(str(file))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        #img = np.expand_dims(img, axis=0)\n",
    "        img_id = file.stem\n",
    "        return img, img_id\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # load image\n",
    "        image, image_id = self.load_sample(self.files[index])\n",
    "        annotation_df = self.gb.get_group(image_id)\n",
    "        #annotation = self.gb.get_group(self.files[index].stem)\n",
    "        #Create a mask\n",
    "        annotations = annotation_df['annotation'].values\n",
    "        mask = build_mask(annotations,input_shape=(520,704), colors=False)\n",
    "        #mask = np.moveaxis(np.array(mask),2,0)\n",
    "            \n",
    "        if self.transform is not None:\n",
    "            augmented = self.transform(image = image, mask = mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "            #mask = self.transform(image = mask)[\"image\"]\n",
    "        \n",
    "        #image = np.expand_dims(image, axis=0)\n",
    "        mask = np.moveaxis(np.array(mask),2,0)\n",
    "        return image,mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82776e0",
   "metadata": {},
   "source": [
    "## Split train-val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ef0abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset to train and val sets\n",
    "train_pics, val_pics = train_test_split(train_files, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446f9510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract pic Id from Path\n",
    "train_pic_id = [path.stem for path in train_pics]\n",
    "val_pic_id = [path.stem for path in val_pics]\n",
    "# Print number of files in tran and val sets\n",
    "print(f'Number of pictures in train set: {len(train_pics)}')\n",
    "print()\n",
    "print(f'Number of pictures in val set: {len(val_pics)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f929f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split pandas dataframe with annotations for train df and val df\n",
    "df_train = cell_df[(cell_df['id'].isin(train_pic_id))]\n",
    "df_val = cell_df[(cell_df['id'].isin(val_pic_id))]\n",
    "\n",
    "print(f\"Number of picture's id in train set {df_train.id.nunique()}\")\n",
    "print()\n",
    "print(f\"Number of picture's id in val set {df_val.id.nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047f77a0",
   "metadata": {},
   "source": [
    "## Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e6c219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augmentation(pic_size,train:bool):\n",
    "    if train == False:\n",
    "        augmentation_test = albumentations.Compose([A.Resize(pic_size,pic_size),\n",
    "                                                    A.Normalize(\n",
    "                                                        mean=[0.485, 0.456, 0.406],\n",
    "                                                        std=[0.229, 0.224, 0.225]),\n",
    "                                                    A.pytorch.transforms.ToTensorV2()])\n",
    "        return augmentation_test\n",
    "    else:\n",
    "        augmentation_train = A.Compose([\n",
    "            A.Resize(pic_size,pic_size),\n",
    "            A.RandomCrop(pic_size, pic_size),\n",
    "            A.OneOf([\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.RandomRotate90(p=0.5),\n",
    "                A.VerticalFlip(p=0.5)], p=0.5),\n",
    "            A.OneOf([\n",
    "                A.GaussNoise(),], p=1),\n",
    "#             A.OneOf([\n",
    "#                 A.MotionBlur(p=0.2),\n",
    "#                 A.MedianBlur(blur_limit=3, p=0.1),\n",
    "#                 A.Blur(blur_limit=3, p=0.1),], p=0.8),\n",
    "#             A.ShiftScaleRotate(\n",
    "#                 shift_limit=0.0625, \n",
    "#                 scale_limit=0.2, \n",
    "#                 rotate_limit=15, p=1),\n",
    "#             A.OneOf([\n",
    "#                 A.OpticalDistortion(p=0.5),\n",
    "#                 A.GridDistortion(p=0.1),], p=0.5),\n",
    "#             A.OneOf([\n",
    "#                 A.CLAHE(clip_limit=2),\n",
    "#                 A.RandomBrightnessContrast(),], p=0.5),\n",
    "#             A.HueSaturationValue(p=0.3),\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]),\n",
    "            A.pytorch.transforms.ToTensorV2()], p=1)\n",
    "        return augmentation_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18510e7d",
   "metadata": {},
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b0c7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaf45f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NeuroDataSet(train_pics,\n",
    "                             df_train,train=True,\n",
    "                             transform=get_augmentation(224, train=True)\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abbd1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=False,\n",
    "                          pin_memory=True,\n",
    "                          num_workers=12,) # pc -8 lap =4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d0a8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = NeuroDataSet(val_pics,\n",
    "                           df_val,\n",
    "                           train=False,\n",
    "                          transform=get_augmentation(224, train=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e10183",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(val_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=False,\n",
    "                          pin_memory=True,\n",
    "                          num_workers=12) # pc -8 lap =4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b421c13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "images, masks = batch\n",
    "print(f\"image shape: {images.shape},\\nmask shape:{masks.shape},\\nbatch len: {len(batch)}\")\n",
    "del batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e487da03",
   "metadata": {},
   "source": [
    "## Visualize Augmented DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76364834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe29721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_augmentations(dataset, idx=1, samples=6, cols=3):\n",
    "    dataset = copy.deepcopy(dataset)\n",
    "    dataset.transform = A.Compose([t for t in dataset.transform if not isinstance(t, (A.Normalize, A.pytorch.ToTensorV2))])\n",
    "    rows = samples // cols\n",
    "    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(12, 12))\n",
    "    for i in range(samples):\n",
    "        image, mask = dataset[idx]\n",
    "        ax.ravel()[i].imshow(image)\n",
    "        ax.ravel()[i].set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebb6208",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_augmentations(train_dataset,idx=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4480c5",
   "metadata": {},
   "source": [
    "# Unet-baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a48938f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create convolution block class\n",
    "class Conv_Block(nn.Module):\n",
    "    '''convolution ==> BN ==> ReLU'''\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4c0ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_block = Conv_Block(3, 64)\n",
    "# x = torch.randn(16,3,224,224)\n",
    "# enc_block(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f4d45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            Conv_Block(in_channels, out_channels),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a9de9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = Encoder(3,128)\n",
    "# # input image\n",
    "# x    = torch.randn(1, 3, 244, 244)\n",
    "# encoder(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1225940a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, kernel_size=2, stride=2)\n",
    "            \n",
    "        self.conv = Conv_Block(in_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "#         # input CxHxW\n",
    "#         diffY = torch.tensor([x2.size()[2] - x1.size()[2]])\n",
    "#         diffX = torch.tensor([x2.size()[3] - x1.size()[3]])\n",
    "        \n",
    "#         x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "#                        diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e71fa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a0ecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "        \n",
    "        self.inc = Conv_Block(n_channels, 64)\n",
    "        self.enc1 = Encoder(64, 128)\n",
    "        self.enc2 = Encoder(128, 256)\n",
    "        self.enc3 = Encoder(256, 512)\n",
    "        self.enc4 = Encoder(512, 512)\n",
    "        self.dec1 = Decoder(1024, 256, bilinear)\n",
    "        self.dec2 = Decoder(512, 128, bilinear)\n",
    "        self.dec3 = Decoder(256, 64, bilinear)\n",
    "        self.dec4 = Decoder(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.enc1(x1)\n",
    "        x3 = self.enc2(x2)\n",
    "        x4 = self.enc3(x3)\n",
    "        x5 = self.enc4(x4)\n",
    "        x = self.dec1(x5, x4)\n",
    "        x = self.dec2(x, x3)\n",
    "        x = self.dec3(x, x2)\n",
    "        x = self.dec4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e863b22",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffda3035",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31da094a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Initial GPU Usage at fit_epoch\")\n",
    "gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44d39a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, optimizer, criterion, train_loader, device=device):\n",
    "    running_loss = 0\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc='Iterating over train data')\n",
    "    \n",
    "#     print(f\"Initial GPU Usage at fit_epoch\")\n",
    "#     gpu_usage()\n",
    "    \n",
    "    for imgs, masks in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        imgs=imgs.to(device).float()\n",
    "        masks = masks.to(device).float()\n",
    "        # forward\n",
    "        \n",
    "        out = model(imgs)\n",
    "        loss = criterion(out, masks)\n",
    "        running_loss += loss.item()*imgs.shape[0]\n",
    "        \n",
    "        loss.backward()\n",
    "#         print(f\"In loop before del\")\n",
    "#         gpu_usage()\n",
    "        del imgs, masks\n",
    "#         print(f\"Afteer deletion in loop\")\n",
    "#         gpu_usage()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    running_loss /= len(train_loader.sampler)\n",
    "    print('Train_loss: %f' % running_loss)\n",
    "    torch.cuda.empty_cache()\n",
    "    #print(f\"Afteer deletion\")\n",
    "    #gpu_usage()\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de5f7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b68fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loop(model, criterion, val_loader, device=device):\n",
    "    running_loss=0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        accuracy, f1_scores = [], []\n",
    "        pbar = tqdm(val_loader, desc='Iterating over evaluation data')\n",
    "        for imgs, masks in pbar:\n",
    "            imgs = imgs.to(device).float()\n",
    "            masks = masks.to(device).float()\n",
    "            # forward\n",
    "            out = model(imgs)\n",
    "            loss = criterion(out, masks)\n",
    "            running_loss += loss.item()*imgs.shape[0]\n",
    "            # calculate predictions using output\n",
    "            \n",
    "            y_hat = (out > 0.5).float()\n",
    "            y_hat = y_hat.view(-1).detach().cpu().numpy()\n",
    "            labels = masks.view(-1).detach().cpu().numpy()\n",
    "            accuracy.append(accuracy_score(labels, y_hat))\n",
    "            f1_scores.append(f1_score(labels, y_hat))\n",
    "            \n",
    "    acc = sum(accuracy)/len(accuracy)\n",
    "    f1 = sum(f1_scores)/len(f1_scores)\n",
    "    running_loss /= len(val_loader.sampler)\n",
    "    print('Val_loss: %f' % running_loss)\n",
    "    del imgs, masks, y_hat, labels\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return {\n",
    "        'accuracy':acc,\n",
    "        'f1_macro':f1, \n",
    "        'loss':running_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c377d7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_train(model,val_loader, device=device):\n",
    "    model.eval()\n",
    "    X_val, Y_val = next(iter(val_loader))\n",
    "    Y_hat = model(X_val.to(device))\n",
    "    Y_hat = Y_hat.detach().cpu().numpy()\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    for k in range(2):\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5),)\n",
    "        ax1.imshow(np.rollaxis(X_val[k].numpy(), 0, 3),cmap='gray')\n",
    "        #ax2.imshow(((Y_hat[k, 0] * 255)).astype('uint8'), cmap='gray')\n",
    "        ax2.imshow(((Y_hat[k, 0] * 255)), cmap='gray')\n",
    "        ax1.set_axis_off()\n",
    "        ax2.set_axis_off()\n",
    "        ax1.set_title('Real')\n",
    "        ax2.set_title('Output')\n",
    "    #plt.suptitle('%d / %d - loss: %f' % (epoch+1, epochs, avg_loss))\n",
    "    plt.show()\n",
    "    del X_val, Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e878ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,optimizer,criterion,\n",
    "          train_loader,val_loader,\n",
    "          device=device,epochs=5,\n",
    "          valid_loss_min=np.inf,vis=True):\n",
    "    writer = SummaryWriter(comment=f'BS_{train_loader.batch_size}_Epchs_{epochs}')\n",
    "    history = []\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_loop(model,optimizer,criterion,train_loader,device=device)\n",
    "        # evaluate on validation set\n",
    "        metrics = eval_loop(model,criterion,val_loader,device=device)\n",
    "        history.append((train_loss,metrics[\"loss\"],metrics[\"accuracy\"]))\n",
    "        \n",
    "        if vis:\n",
    "            visualize_train(model,val_loader,device=device)\n",
    "        \n",
    "        # show progress\n",
    "        print_string = f'Epoch: {epoch+1} '\n",
    "        print_string+= f'TrainLoss from train func: {train_loss:.5f} '\n",
    "        print_string+= f'ValidLoss from train func: {metrics[\"loss\"]:.5f} '\n",
    "        print_string+= f'ACC: {metrics[\"accuracy\"]:.5f} '\n",
    "        print_string+= f'F1: {metrics[\"f1_macro\"]:.3f}'\n",
    "        print(print_string)\n",
    "    \n",
    "    return history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edf0440",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(3,1)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7e5eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://www.kaggle.com/rishabhiitbhu/unet-with-resnet34-encoder-pytorch\n",
    "def dice_loss(input, target):\n",
    "    input = torch.sigmoid(input)\n",
    "    smooth = 1.0\n",
    "    iflat = input.view(-1)\n",
    "    tflat = target.view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "    return ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c3d104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_loss(y_real, y_pred):\n",
    "    # .clamp эквивалентоно Relu\n",
    "    loss = (y_pred.clamp(min=0) - y_pred * y_real + (1 + (-y_pred.abs()).exp()).log()).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b12492",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 10\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f16391",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train(model,optim,dice_loss,train_loader,val_loader,epochs=max_epochs, vis=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f6ee8c",
   "metadata": {},
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7fa1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_acc(history):\n",
    "    \"\"\"Print Loss in train and val sets\"\"\"\n",
    "    train_loss, val_loss, val_acc = zip(*history)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, figsize=(10,10))\n",
    "    fig.suptitle('Loss and Accuracy')\n",
    "    ax1.plot(train_loss, label=\"train_loss\")\n",
    "    ax1.plot(val_loss, label=\"val_loss\")\n",
    "    ax1.legend(loc='best')\n",
    "    plt.ylabel(\"loss\")\n",
    "\n",
    "    #ax2.plot(acc, label=\"train_accuracy\")\n",
    "    ax2.plot(val_acc, label=\"val_accuracy\")\n",
    "    ax2.legend(loc='best')\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f024bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_acc(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66063412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTroch version\n",
    "\n",
    "SMOOTH = 1e-6\n",
    "\n",
    "def iou_pytorch(outputs: torch.Tensor, labels: torch.Tensor):\n",
    "    # You can comment out this line if you are passing tensors of equal shape\n",
    "    # But if you are passing output from UNet or something it will most probably\n",
    "    # be with the BATCH x 1 x H x W shape\n",
    "    outputs = outputs.squeeze(1)  # BATCH x 1 x H x W => BATCH x H x W\n",
    "    \n",
    "    intersection = (outputs & labels).float().sum((1, 2))  # Will be zero if Truth=0 or Prediction=0\n",
    "    union = (outputs | labels).float().sum((1, 2))         # Will be zzero if both are 0\n",
    "    \n",
    "    iou = (intersection + SMOOTH) / (union + SMOOTH)  # We smooth our devision to avoid 0/0\n",
    "    \n",
    "    thresholded = torch.clamp(20 * (iou - 0.5), 0, 10).ceil() / 10  # This is equal to comparing with thresolds\n",
    "    \n",
    "    return thresholded  # Or thresholded.mean() if you are interested in average across the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663ce852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(model, metric, data):\n",
    "    model.eval()  # testing mode\n",
    "    scores = 0\n",
    "    for X_batch, Y_label in data:\n",
    "        Y_pred = model(X_batch.to(device))\n",
    "        Y_label = Y_label.byte()\n",
    "        scores += metric(Y_pred>0.5, Y_label.to(device)).mean().item()\n",
    "\n",
    "    return scores/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163121ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_model(model, iou_pytorch, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54590adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4a68a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c06962e1",
   "metadata": {},
   "source": [
    "# Unet from Segmentation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58415395",
   "metadata": {},
   "source": [
    "https://github.com/qubvel/segmentation_models.pytorch#installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5560832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "from Losses import ComboLoss, dice_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb290c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER          = 'se_resnext50_32x4d'\n",
    "ENCODER_WEIGHTS  = 'imagenet'\n",
    "CLASSES          = ['mask']\n",
    "ACTIVATION       = None\n",
    "CRITERION        = ComboLoss(**{'weights':{'bce':3, 'dice':1, 'focal':4}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067499ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(input, target):\n",
    "    input = torch.sigmoid(input)\n",
    "    smooth = 1.0\n",
    "    iflat = input.view(-1)\n",
    "    tflat = target.view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "    return ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba63df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://www.kaggle.com/rishabhiitbhu/unet-with-resnet34-encoder-pytorch\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if not (target.size() == input.size()):\n",
    "            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n",
    "                             .format(target.size(), input.size()))\n",
    "        max_val = (-input).clamp(min=0)\n",
    "        loss = input - input * target + max_val + \\\n",
    "            ((-max_val).exp() + (-input - max_val).exp()).log()\n",
    "        invprobs = F.logsigmoid(-input * (target * 2.0 - 1.0))\n",
    "        loss = (invprobs * self.gamma).exp() * loss\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf46600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://www.kaggle.com/rishabhiitbhu/unet-with-resnet34-encoder-pytorch\n",
    "\n",
    "class MixedLoss(nn.Module):\n",
    "    def __init__(self, alpha, gamma):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.focal = FocalLoss(gamma)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        loss = self.alpha*self.focal(input, target) - torch.log(dice_loss(input, target))\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4672e8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=len(CLASSES), \n",
    "    activation=ACTIVATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bac174",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044b5831",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 5\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00ac7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train(model,optim,CRITERION,train_loader,val_loader,epochs=max_epochs, vis=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdab77e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9689b80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_acc(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da33ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d498eeed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5957f7e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd7a73f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4636821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d8e19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d069300",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER          = 'se_resnext50_32x4d'\n",
    "ENCODER_WEIGHTS  = 'imagenet'\n",
    "CLASSES          = ['mask']\n",
    "ACTIVATION       = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0062c2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=len(CLASSES), \n",
    "    activation=ACTIVATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd2dc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(input, target):\n",
    "    input = torch.sigmoid(input)\n",
    "    smooth = 1.0\n",
    "    iflat = input.view(-1)\n",
    "    tflat = target.view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "    return ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47360003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_pytorch(outputs: torch.Tensor, labels: torch.Tensor):\n",
    "    # You can comment out this line if you are passing tensors of equal shape\n",
    "    # But if you are passing output from UNet or something it will most probably\n",
    "    # be with the BATCH x 1 x H x W shape\n",
    "    outputs = outputs.squeeze(1).byte()  # BATCH x 1 x H x W => BATCH x H x W\n",
    "    labels = labels.squeeze(1).byte()\n",
    "    SMOOTH = 1e-8\n",
    "    intersection = (outputs & labels).float().sum((1, 2))  # Will be zero if Truth=0 or Prediction=0\n",
    "    union = (outputs | labels).float().sum((1, 2))         # Will be zzero if both are 0\n",
    "    \n",
    "    iou = (intersection + SMOOTH) / (union + SMOOTH)  # We smooth our devision to avoid 0/0\n",
    "    \n",
    "    thresholded = torch.clamp(20 * (iou - 0.5), 0, 10).ceil() / 10  # This is equal to comparing with thresolds\n",
    "    \n",
    "    return thresholded  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b4c007",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d27c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409cc99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, opt, loss_fn, epochs, data_tr, data_val):\n",
    "    X_val, Y_val = next(iter(data_val))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('* Epoch %d/%d' % (epoch+1, epochs))\n",
    "\n",
    "        avg_loss = 0\n",
    "        model.train()  # train mode\n",
    "        for X_batch, Y_batch in data_tr:\n",
    "\n",
    "          X_batch = X_batch.to(device)\n",
    "          Y_batch = Y_batch.to(device)\n",
    "          opt.zero_grad()\n",
    "           \n",
    "          Y_pred = model(X_batch)\n",
    "          loss = loss_fn(Y_batch, Y_pred)\n",
    "          loss.backward()  # backward-pass\n",
    "          opt.step()   # update weights\n",
    "          torch.cuda.empty_cache()\n",
    "\n",
    "            # calculate loss to show the user\n",
    "          avg_loss += loss / len(data_tr)\n",
    "        print('loss: %f' % avg_loss)\n",
    "\n",
    "        # show intermediate results\n",
    "        model.eval()  # testing mode\n",
    "        Y_hat = model(X_val.to(device))\n",
    "        # detach and put into cpu\n",
    "        Y_hat = Y_hat.detach().cpu().numpy()\n",
    "\n",
    "        # Visualize tools\n",
    "        clear_output(wait=True)\n",
    "        torch.cuda.empty_cache()\n",
    "        for k in range(6):\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(30, 20),)\n",
    "            plt.subplot(2, 6, k+1, )#figsize=(15, 15))\n",
    "            plt.imshow(np.rollaxis(X_val[k].numpy(), 0, 3), cmap='gray')\n",
    "            plt.title('Real')\n",
    "            plt.axis('off')\n",
    "\n",
    "            plt.subplot(2, 6, k+7,)#figsize=(15, 15))\n",
    "            plt.imshow(Y_hat[k, 0], cmap='gray')\n",
    "            plt.title('Output')\n",
    "            plt.axis('off')\n",
    "        plt.suptitle('%d / %d - loss: %f' % (epoch+1, epochs, avg_loss))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15ffae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f02276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data):\n",
    "    model.eval()  # testing mode\n",
    "    Y_pred = [ X_batch for X_batch, _ in data]\n",
    "    return np.array(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263395a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(model, metric, data):\n",
    "    model.eval()  # testing mode\n",
    "    scores = 0\n",
    "    for X_batch, Y_label in data:\n",
    "        Y_pred = model(X_batch.to(device))\n",
    "        scores += metric(Y_pred>0.5, Y_label.to(device)).mean().item()\n",
    "\n",
    "    return scores/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df92f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(3,1)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9579d94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 5\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "train(model, optim, dice_loss, max_epochs, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3c7e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_model(model, iou_pytorch, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055d3a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4fca42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
